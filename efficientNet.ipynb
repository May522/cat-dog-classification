{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficientnet_b3迁移学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision import datasets,transforms,models\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\King/.cache\\torch\\hub\\rwightman_gen-efficientnet-pytorch_master\n",
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b3_ra-a5e2fbc7.pth\" to C:\\Users\\King/.cache\\torch\\checkpoints\\efficientnet_b3_ra-a5e2fbc7.pth\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('rwightman/gen-efficientnet-pytorch',\n",
    "                       'efficientnet_b3', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform=transforms.Compose([\n",
    "    transforms.Resize(320),\n",
    "    transforms.CenterCrop(320),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.488,0.455,0.417],std=[0.230,0.225,0.225])#RGB\n",
    "    #transforms.Normalize(mean=[0.415,0.454,0.487],std=[0.225,0.225,0.230])#\n",
    "])\n",
    "#训练数据读取接口\n",
    "train_dataset=datasets.ImageFolder(root='./copy/train/',\n",
    "                                  transform= data_transform)\n",
    "#把数据打包成batch_size大小的tensor\n",
    "train_loader=torch.utils.data.DataLoader( train_dataset,\n",
    "                                        batch_size=64,\n",
    "                                        shuffle=True)\n",
    "#验证集数据读取接口\n",
    "val_dataset=datasets.ImageFolder(root='./copy/val/',\n",
    "                                 transform=data_transform)\n",
    "val_loader=torch.utils.data.DataLoader(val_dataset,\n",
    "                                       batch_size=64,\n",
    "                                       shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cats': 0, 'dogs': 1}\n"
     ]
    }
   ],
   "source": [
    "print(val_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier=nn.Sequential(nn.Linear(1280,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.cuda()\n",
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.classifier.parameters(),lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val():\n",
    "    correct=0\n",
    "    val_loss=0.0\n",
    "    val_total=0\n",
    "    model.eval()\n",
    "\n",
    "    for i,data in enumerate(val_loader):\n",
    "        images,labels=data\n",
    "        images,labels=Variable(images.cuda()),Variable(labels.cuda())\n",
    "        outputs=model(images)\n",
    "        _,predicted=torch.max(outputs.data,1)\n",
    "        loss=criterion(outputs,labels)\n",
    "        val_loss+=loss.item()\n",
    "        val_total+=labels.size(0)\n",
    "        correct+=(predicted==labels.data).sum()\n",
    "\n",
    "#     print('val %d epoch loss: %.3f  acc:%4.2f ' %(epoch+1,\n",
    "#                                                  val_loss/(i+1),\n",
    "#                                                  100*float(correct)/float(val_total)))\n",
    "    return val_loss/(i+1),100*float(correct)/float(val_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss=0.0\n",
    "        train_correct=0\n",
    "        train_total=0\n",
    "        \n",
    "        for i,data in enumerate(train_loader,0):#一个epoch完成后结束for循环\n",
    "            inputs,train_labels=data\n",
    "            inputs,labels=Variable(inputs.cuda()),Variable(train_labels.cuda())\n",
    "            outputs=model(inputs)#一个batch的输出\n",
    "            \n",
    "            #根据网络输出计算此时的准确率\n",
    "            _,train_predicted=torch.max(outputs.data,1)\n",
    "            train_correct+=(train_predicted==labels.data).sum()#每个batch中正确的个数，累加器\n",
    "            \n",
    "            loss=criterion(outputs,labels)\n",
    "            \n",
    "            #计算损失函数中的L2正则化项\n",
    "#             L2_reg=0\n",
    "#             for param in model.parameters():\n",
    "#                 L2_reg+=torch.norm(param)\n",
    "            \n",
    "#             loss+=0.1*L2_reg\n",
    "            \n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss+=loss.item() #损失值，累加器\n",
    "            train_total+=train_labels.size(0)#batch图像个数，累加器\n",
    "            \n",
    "            if (i+1)%print_per_batches==0: #每隔10个batch就记录一次这10个batch的平均损失值\n",
    "                val_loss,val_acc=val()\n",
    "                print('[%d %5d] train loss: %.3f  train acc: %.3f  val loss: %.3f  val acc: %.3f'\n",
    "                      %(epoch+1,i+1,running_loss/print_per_batches,100*float(train_correct)/float(train_total),val_loss,val_acc))\n",
    "                train_loss_batch.append(running_loss/print_per_batches)\n",
    "                val_loss_batch.append(val_loss)\n",
    "                running_loss=0\n",
    "                train_correct=0\n",
    "                train_total=0\n",
    "                \n",
    "               \n",
    "        \n",
    "        #print('train %d epoch  acc: %4.2f'%(epoch+1, 100*float(train_correct)/float(train_total)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #在训练完一个epoch后，用验证集进行验证\n",
    "        \n",
    "        \n",
    "        if epoch % save_every==0:\n",
    "            torch.save(model.state_dict(),'./models/model-%d.pkl' % epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.234071493148804\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "val_loss,val_acc=val()\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1    20] train loss: 0.296  train acc: 93.906  val loss: 0.286  val acc: 94.360\n",
      "[1    40] train loss: 0.281  train acc: 94.219  val loss: 0.274  val acc: 94.600\n",
      "[1    60] train loss: 0.262  train acc: 95.000  val loss: 0.262  val acc: 94.680\n",
      "[1    80] train loss: 0.261  train acc: 94.609  val loss: 0.255  val acc: 94.720\n",
      "[1   100] train loss: 0.259  train acc: 93.438  val loss: 0.246  val acc: 94.920\n",
      "[1   120] train loss: 0.245  train acc: 95.234  val loss: 0.243  val acc: 94.920\n",
      "[1   140] train loss: 0.238  train acc: 93.984  val loss: 0.236  val acc: 94.880\n",
      "[1   160] train loss: 0.231  train acc: 94.062  val loss: 0.225  val acc: 94.880\n",
      "[1   180] train loss: 0.221  train acc: 95.625  val loss: 0.222  val acc: 95.200\n",
      "[1   200] train loss: 0.223  train acc: 95.078  val loss: 0.219  val acc: 95.000\n",
      "[1   220] train loss: 0.204  train acc: 95.781  val loss: 0.209  val acc: 95.200\n",
      "[1   240] train loss: 0.200  train acc: 96.562  val loss: 0.207  val acc: 95.160\n",
      "[1   260] train loss: 0.203  train acc: 95.859  val loss: 0.200  val acc: 95.240\n",
      "[1   280] train loss: 0.202  train acc: 95.625  val loss: 0.199  val acc: 95.320\n",
      "[1   300] train loss: 0.191  train acc: 95.703  val loss: 0.202  val acc: 95.400\n",
      "[1   320] train loss: 0.185  train acc: 95.312  val loss: 0.193  val acc: 95.600\n",
      "[1   340] train loss: 0.181  train acc: 95.547  val loss: 0.191  val acc: 95.480\n",
      "[2    20] train loss: 0.182  train acc: 95.547  val loss: 0.184  val acc: 95.360\n",
      "[2    40] train loss: 0.177  train acc: 96.016  val loss: 0.183  val acc: 95.440\n",
      "[2    60] train loss: 0.178  train acc: 95.938  val loss: 0.179  val acc: 95.480\n",
      "[2    80] train loss: 0.176  train acc: 96.172  val loss: 0.174  val acc: 95.440\n",
      "[2   100] train loss: 0.175  train acc: 95.781  val loss: 0.174  val acc: 95.680\n",
      "[2   120] train loss: 0.163  train acc: 96.250  val loss: 0.174  val acc: 95.640\n",
      "[2   140] train loss: 0.175  train acc: 95.781  val loss: 0.168  val acc: 95.680\n",
      "[2   160] train loss: 0.167  train acc: 95.312  val loss: 0.166  val acc: 95.720\n",
      "[2   180] train loss: 0.167  train acc: 95.625  val loss: 0.164  val acc: 95.760\n",
      "[2   200] train loss: 0.164  train acc: 95.781  val loss: 0.162  val acc: 95.760\n",
      "[2   220] train loss: 0.164  train acc: 96.016  val loss: 0.159  val acc: 95.840\n",
      "[2   240] train loss: 0.143  train acc: 96.406  val loss: 0.157  val acc: 95.800\n",
      "[2   260] train loss: 0.156  train acc: 95.469  val loss: 0.158  val acc: 95.840\n",
      "[2   280] train loss: 0.147  train acc: 96.016  val loss: 0.161  val acc: 95.920\n",
      "[2   300] train loss: 0.145  train acc: 96.016  val loss: 0.152  val acc: 95.920\n",
      "[2   320] train loss: 0.153  train acc: 95.703  val loss: 0.154  val acc: 95.840\n",
      "[2   340] train loss: 0.151  train acc: 95.859  val loss: 0.150  val acc: 95.920\n",
      "[3    20] train loss: 0.157  train acc: 95.391  val loss: 0.153  val acc: 95.720\n",
      "[3    40] train loss: 0.145  train acc: 96.484  val loss: 0.150  val acc: 95.760\n",
      "[3    60] train loss: 0.147  train acc: 95.547  val loss: 0.148  val acc: 95.800\n",
      "[3    80] train loss: 0.160  train acc: 95.312  val loss: 0.146  val acc: 95.760\n",
      "[3   100] train loss: 0.137  train acc: 96.406  val loss: 0.149  val acc: 95.800\n",
      "[3   120] train loss: 0.142  train acc: 96.094  val loss: 0.150  val acc: 95.680\n",
      "[3   140] train loss: 0.132  train acc: 96.719  val loss: 0.144  val acc: 95.880\n",
      "[3   160] train loss: 0.142  train acc: 95.547  val loss: 0.143  val acc: 95.960\n",
      "[3   180] train loss: 0.128  train acc: 96.875  val loss: 0.150  val acc: 95.880\n",
      "[3   200] train loss: 0.148  train acc: 95.938  val loss: 0.138  val acc: 96.000\n",
      "[3   220] train loss: 0.125  train acc: 96.641  val loss: 0.138  val acc: 96.040\n",
      "[3   240] train loss: 0.131  train acc: 96.250  val loss: 0.137  val acc: 96.040\n",
      "[3   260] train loss: 0.143  train acc: 95.625  val loss: 0.139  val acc: 95.960\n",
      "[3   280] train loss: 0.136  train acc: 96.016  val loss: 0.136  val acc: 96.000\n",
      "[3   300] train loss: 0.128  train acc: 96.172  val loss: 0.138  val acc: 96.000\n",
      "[3   320] train loss: 0.136  train acc: 95.781  val loss: 0.135  val acc: 96.000\n",
      "[3   340] train loss: 0.123  train acc: 96.250  val loss: 0.134  val acc: 95.960\n",
      "[4    20] train loss: 0.139  train acc: 95.547  val loss: 0.133  val acc: 95.960\n",
      "[4    40] train loss: 0.128  train acc: 95.938  val loss: 0.130  val acc: 96.200\n",
      "[4    60] train loss: 0.118  train acc: 96.328  val loss: 0.135  val acc: 96.040\n",
      "[4    80] train loss: 0.128  train acc: 97.031  val loss: 0.131  val acc: 96.040\n",
      "[4   100] train loss: 0.132  train acc: 95.859  val loss: 0.129  val acc: 96.160\n",
      "[4   120] train loss: 0.122  train acc: 96.562  val loss: 0.135  val acc: 96.160\n",
      "[4   140] train loss: 0.122  train acc: 96.484  val loss: 0.126  val acc: 96.160\n",
      "[4   160] train loss: 0.119  train acc: 96.875  val loss: 0.130  val acc: 96.240\n",
      "[4   180] train loss: 0.110  train acc: 97.266  val loss: 0.135  val acc: 96.200\n",
      "[4   200] train loss: 0.117  train acc: 96.328  val loss: 0.125  val acc: 96.200\n",
      "[4   220] train loss: 0.132  train acc: 95.469  val loss: 0.125  val acc: 96.200\n",
      "[4   240] train loss: 0.123  train acc: 96.406  val loss: 0.137  val acc: 96.160\n",
      "[4   260] train loss: 0.114  train acc: 97.188  val loss: 0.124  val acc: 96.160\n",
      "[4   280] train loss: 0.098  train acc: 97.578  val loss: 0.124  val acc: 96.160\n",
      "[4   300] train loss: 0.126  train acc: 95.547  val loss: 0.122  val acc: 96.160\n",
      "[4   320] train loss: 0.122  train acc: 96.016  val loss: 0.121  val acc: 96.160\n",
      "[4   340] train loss: 0.114  train acc: 96.016  val loss: 0.124  val acc: 96.120\n",
      "[5    20] train loss: 0.115  train acc: 96.953  val loss: 0.125  val acc: 96.400\n",
      "[5    40] train loss: 0.111  train acc: 96.406  val loss: 0.119  val acc: 96.400\n",
      "[5    60] train loss: 0.117  train acc: 95.938  val loss: 0.121  val acc: 96.440\n",
      "[5    80] train loss: 0.105  train acc: 97.656  val loss: 0.118  val acc: 96.320\n",
      "[5   100] train loss: 0.101  train acc: 97.344  val loss: 0.122  val acc: 96.280\n",
      "[5   120] train loss: 0.116  train acc: 96.094  val loss: 0.118  val acc: 96.320\n",
      "[5   140] train loss: 0.117  train acc: 96.016  val loss: 0.122  val acc: 96.400\n",
      "[5   160] train loss: 0.106  train acc: 97.109  val loss: 0.123  val acc: 96.360\n",
      "[5   180] train loss: 0.115  train acc: 96.094  val loss: 0.117  val acc: 96.400\n",
      "[5   200] train loss: 0.124  train acc: 95.781  val loss: 0.116  val acc: 96.360\n",
      "[5   220] train loss: 0.109  train acc: 96.953  val loss: 0.120  val acc: 96.360\n",
      "[5   240] train loss: 0.105  train acc: 96.562  val loss: 0.115  val acc: 96.280\n",
      "[5   260] train loss: 0.097  train acc: 97.266  val loss: 0.117  val acc: 96.400\n",
      "[5   280] train loss: 0.093  train acc: 97.422  val loss: 0.115  val acc: 96.400\n",
      "[5   300] train loss: 0.102  train acc: 96.797  val loss: 0.114  val acc: 96.480\n",
      "[5   320] train loss: 0.101  train acc: 97.031  val loss: 0.115  val acc: 96.400\n",
      "[5   340] train loss: 0.124  train acc: 95.156  val loss: 0.113  val acc: 96.360\n",
      "time used : 101.40095043182373 minutes\n"
     ]
    }
   ],
   "source": [
    "train_loss_batch=[]# 记录训练过程中的损失值\n",
    "val_loss_batch=[]\n",
    "save_every=1 #每一个epoch保存一下网络参数值\n",
    "epochs=5 \n",
    "print_per_batches=20\n",
    "\n",
    "# 开始训练，并记录训练消耗的时间\n",
    "time_open=time.time()\n",
    "train()\n",
    "time_end=time.time()\n",
    "time_used=time_end-time_open\n",
    "print('time used : {} minutes'.format(time_used/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=range(1,len(train_loss_batch)+1)\n",
    "plt.plot(x,train_loss_batch,label='train loss')\n",
    "plt.plot(x,val_loss_batch,label='val loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('efficient_trainLoss.txt','w')\n",
    "f.write(str(train_loss_batch))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('efficient_valLoss.txt','w')\n",
    "f.write(str(val_loss_batch))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
