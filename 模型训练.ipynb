{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 随机初始化的alexnet网络训练 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision import datasets,transforms,models\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据paper自己定义的elexnet网络，共8层，5加3，不含dropout层\n",
    "class myNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,num_classes=2):\n",
    "        super(myNet,self).__init__()\n",
    "        self.features=nn.Sequential(nn.Conv2d(3,96,11,4,padding=5),\n",
    "                                   nn.BatchNorm2d(96),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "                                   nn.Conv2d(96,256,5,padding=2),\n",
    "                                   nn.BatchNorm2d(256),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "                                   nn.Conv2d(256,384,3,padding=1),\n",
    "                                   nn.BatchNorm2d(384),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.Conv2d(384,384,3,padding=1),\n",
    "                                   nn.BatchNorm2d(384),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.Conv2d(384,256,3,padding=1),\n",
    "                                   nn.BatchNorm2d(256),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "                                   )\n",
    "#         self.classifier=nn.Sequential(#nn.Dropout(p=0.5),\n",
    "#                                      nn.Linear(256*6*6,4096),\n",
    "#                                      nn.ReLU(inplace=True),\n",
    "#                                      #nn.Dropout(p=0.5),\n",
    "#                                      nn.Linear(4096,4096),\n",
    "#                                      nn.ReLU(inplace=True),\n",
    "#                                      nn.Linear(4096,num_classes),\n",
    "#                                      )\n",
    "        self.classifier=nn.Sequential(#nn.Dropout(p=0.5),\n",
    "                                     nn.Linear(256*6*6,4096),\n",
    "                                     nn.ReLU(inplace=True),\n",
    "                                     #nn.Dropout(p=0.5),\n",
    "                                     nn.Linear(4096,4096),\n",
    "                                     nn.ReLU(inplace=True),\n",
    "                                     nn.Linear(4096,num_classes),\n",
    "                                     )\n",
    "    def forward(self,x):\n",
    "        x=self.features(x)\n",
    "        x=x.view(x.size(0),256*6*6)\n",
    "        x=self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform=transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.488,0.455,0.417],std=[0.230,0.225,0.225])#RGB\n",
    "    #transforms.Normalize(mean=[0.415,0.454,0.487],std=[0.225,0.225,0.230])#\n",
    "])\n",
    "#训练数据读取接口\n",
    "train_dataset=datasets.ImageFolder(root='./copy/train/',\n",
    "                                  transform= data_transform)\n",
    "#把数据打包成batch_size大小的tensor\n",
    "train_loader=torch.utils.data.DataLoader( train_dataset,\n",
    "                                        batch_size=64,\n",
    "                                        shuffle=True)\n",
    "#验证集数据读取接口\n",
    "val_dataset=datasets.ImageFolder(root='./copy/val/',\n",
    "                                 transform=data_transform)\n",
    "val_loader=torch.utils.data.DataLoader(val_dataset,\n",
    "                                       batch_size=64,\n",
    "                                       shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cats': 0, 'dogs': 1}\n"
     ]
    }
   ],
   "source": [
    "print(val_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=myNet()\n",
    "model=model.cuda()\n",
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val():\n",
    "    correct=0\n",
    "    val_loss=0.0\n",
    "    val_total=0\n",
    "    model.eval()\n",
    "\n",
    "    for i,data in enumerate(val_loader):\n",
    "        images,labels=data\n",
    "        images,labels=Variable(images.cuda()),Variable(labels.cuda())\n",
    "        outputs=model(images)\n",
    "        _,predicted=torch.max(outputs.data,1)\n",
    "        loss=criterion(outputs,labels)\n",
    "        val_loss+=loss.item()\n",
    "        val_total+=labels.size(0)\n",
    "        correct+=(predicted==labels.data).sum()\n",
    "        \n",
    "    return val_loss/(i+1),100*float(correct)/float(val_total)\n",
    "\n",
    "\n",
    "def train():\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss=0.0\n",
    "        train_correct=0\n",
    "        train_total=0\n",
    "        \n",
    "        for i,data in enumerate(train_loader,0):#一个epoch完成后结束for循环\n",
    "            inputs,train_labels=data\n",
    "            inputs,labels=Variable(inputs.cuda()),Variable(train_labels.cuda())\n",
    "            outputs=model(inputs)#一个batch的输出\n",
    "            \n",
    "            #根据网络输出计算此时的准确率\n",
    "            _,train_predicted=torch.max(outputs.data,1)\n",
    "            train_correct+=(train_predicted==labels.data).sum()#每个batch中正确的个数，累加器\n",
    "            \n",
    "            loss=criterion(outputs,labels)\n",
    "            \n",
    "            #计算损失函数中的L2正则化项\n",
    "#             L2_reg=0\n",
    "#             for param in model.parameters():\n",
    "#                 L2_reg+=torch.norm(param)\n",
    "            \n",
    "#             loss+=0.1*L2_reg\n",
    "            \n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss+=loss.item() #损失值，累加器\n",
    "            train_total+=train_labels.size(0)#batch图像个数，累加器\n",
    "            \n",
    "            if (i+1)%print_per_batches==0: #每隔10个batch就记录一次这10个batch的平均损失值\n",
    "                val_loss,val_acc=val()\n",
    "                print('[%d %5d] train loss: %.3f  train acc: %.3f  val loss: %.3f  val acc: %.3f'\n",
    "                      %(epoch+1,i+1,running_loss/print_per_batches,100*float(train_correct)/float(train_total),val_loss,val_acc))\n",
    "                train_loss_batch.append(running_loss/print_per_batches)\n",
    "                val_loss_batch.append(val_loss)\n",
    "                running_loss=0\n",
    "                train_correct=0\n",
    "                train_total=0\n",
    "\n",
    "        if epoch % save_every==0:\n",
    "            torch.save(model.state_dict(),'./models/model-%d.pkl' % epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1    20] train loss: 0.905  train acc: 53.828  val loss: 0.659  val acc: 61.040\n",
      "[1    40] train loss: 0.663  train acc: 60.469  val loss: 0.645  val acc: 63.000\n",
      "[1    60] train loss: 0.638  train acc: 62.656  val loss: 0.662  val acc: 61.960\n",
      "[1    80] train loss: 0.645  train acc: 63.125  val loss: 0.645  val acc: 62.960\n",
      "[1   100] train loss: 0.629  train acc: 64.297  val loss: 0.622  val acc: 67.760\n",
      "[1   120] train loss: 0.627  train acc: 63.516  val loss: 0.642  val acc: 62.560\n",
      "[1   140] train loss: 0.586  train acc: 69.297  val loss: 0.565  val acc: 70.760\n",
      "[1   160] train loss: 0.595  train acc: 68.125  val loss: 0.566  val acc: 71.640\n",
      "[1   180] train loss: 0.607  train acc: 67.422  val loss: 0.564  val acc: 71.080\n",
      "[1   200] train loss: 0.570  train acc: 71.172  val loss: 0.581  val acc: 71.840\n",
      "[1   220] train loss: 0.570  train acc: 71.719  val loss: 0.560  val acc: 71.680\n",
      "[1   240] train loss: 0.519  train acc: 75.078  val loss: 0.537  val acc: 72.800\n",
      "[1   260] train loss: 0.528  train acc: 75.078  val loss: 0.552  val acc: 72.320\n",
      "[1   280] train loss: 0.573  train acc: 69.922  val loss: 0.561  val acc: 71.600\n",
      "[1   300] train loss: 0.542  train acc: 73.438  val loss: 0.499  val acc: 75.280\n",
      "[1   320] train loss: 0.520  train acc: 74.609  val loss: 0.507  val acc: 74.360\n",
      "[1   340] train loss: 0.480  train acc: 78.281  val loss: 0.504  val acc: 75.640\n",
      "[2    20] train loss: 0.498  train acc: 76.172  val loss: 0.487  val acc: 76.840\n",
      "[2    40] train loss: 0.514  train acc: 75.859  val loss: 0.494  val acc: 76.560\n",
      "[2    60] train loss: 0.513  train acc: 75.859  val loss: 0.548  val acc: 73.320\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ddec311c9f26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# 开始训练，并记录训练消耗的时间\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtime_open\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mtime_end\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtime_used\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime_end\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtime_open\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-6d655a93607d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[0mrunning_loss\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#损失值，累加器\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m             \u001b[0mtrain_total\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#batch图像个数，累加器\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss_batch=[]# 记录训练过程中的损失值\n",
    "val_loss_batch=[]\n",
    "save_every=1 #每一个epoch保存一下网络参数值\n",
    "epochs=5 \n",
    "print_per_batches=20\n",
    "\n",
    "# 开始训练，并记录训练消耗的时间\n",
    "time_open=time.time()\n",
    "train()\n",
    "time_end=time.time()\n",
    "time_used=time_end-time_open\n",
    "print('time used : {} minutes'.format(time_used/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出训练集的损失函数曲线图\n",
    "x=range(1,len(train_loss_batch)+1)\n",
    "plt.plot(x,train_loss_batch,label='train loss')\n",
    "plt.plot(x,val_loss_batch,label='val loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 用Alexnet网络迁移学习\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=models.alexnet(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "model.classifier=nn.Sequential(nn.Dropout(p=0.5),\n",
    "                               nn.Linear(256*6*6,4096),\n",
    "                               nn.ReLU(inplace=True),\n",
    "                               nn.Dropout(p=0.5),\n",
    "                               nn.Linear(4096,4096),\n",
    "                               nn.ReLU(inplace=True),\n",
    "                               nn.Linear(4096,2),\n",
    "                              )\n",
    "model=model.cuda()\n",
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.classifier.parameters(),lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1    10] loss: 0.362\n",
      "[1    20] loss: 0.206\n",
      "[1    30] loss: 0.213\n",
      "[1    40] loss: 0.245\n",
      "[1    50] loss: 0.235\n",
      "[1    60] loss: 0.174\n",
      "[1    70] loss: 0.223\n",
      "[1    80] loss: 0.147\n",
      "[1    90] loss: 0.166\n",
      "[1   100] loss: 0.171\n",
      "[1   110] loss: 0.185\n",
      "[1   120] loss: 0.154\n",
      "[1   130] loss: 0.152\n",
      "[1   140] loss: 0.139\n",
      "[1   150] loss: 0.174\n",
      "[1   160] loss: 0.132\n",
      "[1   170] loss: 0.152\n",
      "[1   180] loss: 0.140\n",
      "[1   190] loss: 0.160\n",
      "[1   200] loss: 0.128\n",
      "[1   210] loss: 0.164\n",
      "[1   220] loss: 0.148\n",
      "[1   230] loss: 0.135\n",
      "[1   240] loss: 0.166\n",
      "[1   250] loss: 0.098\n",
      "[1   260] loss: 0.125\n",
      "[1   270] loss: 0.119\n",
      "[1   280] loss: 0.110\n",
      "[1   290] loss: 0.107\n",
      "[1   300] loss: 0.102\n",
      "[1   310] loss: 0.080\n",
      "[1   320] loss: 0.123\n",
      "[1   330] loss: 0.119\n",
      "[1   340] loss: 0.122\n",
      "[1   350] loss: 0.152\n",
      "train 1 epoch  acc: 93.85\n",
      "correct : tensor(2384, device='cuda:0')  val_total : 2500\n",
      "val 1 epoch loss: 0.123  acc:95.36 \n",
      "[2    10] loss: 0.080\n",
      "[2    20] loss: 0.087\n",
      "[2    30] loss: 0.095\n",
      "[2    40] loss: 0.107\n",
      "[2    50] loss: 0.105\n",
      "[2    60] loss: 0.115\n",
      "[2    70] loss: 0.129\n",
      "[2    80] loss: 0.102\n",
      "[2    90] loss: 0.085\n",
      "[2   100] loss: 0.079\n",
      "[2   110] loss: 0.106\n",
      "[2   120] loss: 0.119\n",
      "[2   130] loss: 0.113\n",
      "[2   140] loss: 0.124\n",
      "[2   150] loss: 0.125\n",
      "[2   160] loss: 0.120\n",
      "[2   170] loss: 0.087\n",
      "[2   180] loss: 0.111\n",
      "[2   190] loss: 0.081\n",
      "[2   200] loss: 0.110\n",
      "[2   210] loss: 0.109\n",
      "[2   220] loss: 0.105\n",
      "[2   230] loss: 0.090\n",
      "[2   240] loss: 0.121\n",
      "[2   250] loss: 0.114\n",
      "[2   260] loss: 0.135\n",
      "[2   270] loss: 0.140\n",
      "[2   280] loss: 0.091\n",
      "[2   290] loss: 0.109\n",
      "[2   300] loss: 0.094\n",
      "[2   310] loss: 0.091\n",
      "[2   320] loss: 0.112\n",
      "[2   330] loss: 0.107\n",
      "[2   340] loss: 0.116\n",
      "[2   350] loss: 0.093\n",
      "train 2 epoch  acc: 95.85\n",
      "correct : tensor(2392, device='cuda:0')  val_total : 2500\n",
      "val 2 epoch loss: 0.103  acc:95.68 \n",
      "[3    10] loss: 0.106\n",
      "[3    20] loss: 0.086\n",
      "[3    30] loss: 0.065\n",
      "[3    40] loss: 0.107\n",
      "[3    50] loss: 0.078\n",
      "[3    60] loss: 0.069\n",
      "[3    70] loss: 0.084\n",
      "[3    80] loss: 0.086\n",
      "[3    90] loss: 0.087\n",
      "[3   100] loss: 0.056\n",
      "[3   110] loss: 0.088\n",
      "[3   120] loss: 0.074\n",
      "[3   130] loss: 0.116\n",
      "[3   140] loss: 0.071\n",
      "[3   150] loss: 0.090\n",
      "[3   160] loss: 0.100\n",
      "[3   170] loss: 0.075\n",
      "[3   180] loss: 0.087\n",
      "[3   190] loss: 0.082\n",
      "[3   200] loss: 0.095\n",
      "[3   210] loss: 0.092\n",
      "[3   220] loss: 0.076\n",
      "[3   230] loss: 0.105\n",
      "[3   240] loss: 0.104\n",
      "[3   250] loss: 0.104\n",
      "[3   260] loss: 0.101\n",
      "[3   270] loss: 0.100\n",
      "[3   280] loss: 0.085\n",
      "[3   290] loss: 0.091\n",
      "[3   300] loss: 0.082\n",
      "[3   310] loss: 0.097\n",
      "[3   320] loss: 0.112\n",
      "[3   330] loss: 0.096\n",
      "[3   340] loss: 0.096\n",
      "[3   350] loss: 0.085\n",
      "train 3 epoch  acc: 96.44\n",
      "correct : tensor(2396, device='cuda:0')  val_total : 2500\n",
      "val 3 epoch loss: 0.098  acc:95.84 \n",
      "[4    10] loss: 0.097\n",
      "[4    20] loss: 0.076\n",
      "[4    30] loss: 0.056\n",
      "[4    40] loss: 0.061\n",
      "[4    50] loss: 0.059\n",
      "[4    60] loss: 0.088\n",
      "[4    70] loss: 0.105\n",
      "[4    80] loss: 0.073\n",
      "[4    90] loss: 0.072\n",
      "[4   100] loss: 0.064\n",
      "[4   110] loss: 0.081\n",
      "[4   120] loss: 0.065\n",
      "[4   130] loss: 0.074\n",
      "[4   140] loss: 0.122\n",
      "[4   150] loss: 0.095\n",
      "[4   160] loss: 0.087\n",
      "[4   170] loss: 0.090\n",
      "[4   180] loss: 0.087\n",
      "[4   190] loss: 0.108\n",
      "[4   200] loss: 0.070\n",
      "[4   210] loss: 0.093\n",
      "[4   220] loss: 0.068\n",
      "[4   230] loss: 0.062\n",
      "[4   240] loss: 0.098\n",
      "[4   250] loss: 0.078\n",
      "[4   260] loss: 0.090\n",
      "[4   270] loss: 0.074\n",
      "[4   280] loss: 0.063\n",
      "[4   290] loss: 0.064\n",
      "[4   300] loss: 0.063\n",
      "[4   310] loss: 0.095\n",
      "[4   320] loss: 0.095\n",
      "[4   330] loss: 0.068\n",
      "[4   340] loss: 0.084\n",
      "[4   350] loss: 0.117\n",
      "train 4 epoch  acc: 96.76\n",
      "correct : tensor(2401, device='cuda:0')  val_total : 2500\n",
      "val 4 epoch loss: 0.100  acc:96.04 \n",
      "[5    10] loss: 0.077\n",
      "[5    20] loss: 0.092\n",
      "[5    30] loss: 0.077\n",
      "[5    40] loss: 0.088\n",
      "[5    50] loss: 0.085\n",
      "[5    60] loss: 0.054\n",
      "[5    70] loss: 0.077\n",
      "[5    80] loss: 0.073\n",
      "[5    90] loss: 0.061\n",
      "[5   100] loss: 0.060\n",
      "[5   110] loss: 0.082\n",
      "[5   120] loss: 0.068\n",
      "[5   130] loss: 0.066\n",
      "[5   140] loss: 0.040\n",
      "[5   150] loss: 0.059\n",
      "[5   160] loss: 0.056\n",
      "[5   170] loss: 0.033\n",
      "[5   180] loss: 0.048\n",
      "[5   190] loss: 0.076\n",
      "[5   200] loss: 0.055\n",
      "[5   210] loss: 0.063\n",
      "[5   220] loss: 0.075\n",
      "[5   230] loss: 0.062\n",
      "[5   240] loss: 0.080\n",
      "[5   250] loss: 0.079\n",
      "[5   260] loss: 0.064\n",
      "[5   270] loss: 0.070\n",
      "[5   280] loss: 0.079\n",
      "[5   290] loss: 0.090\n",
      "[5   300] loss: 0.068\n",
      "[5   310] loss: 0.045\n",
      "[5   320] loss: 0.087\n",
      "[5   330] loss: 0.073\n",
      "[5   340] loss: 0.066\n",
      "[5   350] loss: 0.047\n",
      "train 5 epoch  acc: 97.29\n",
      "correct : tensor(2408, device='cuda:0')  val_total : 2500\n",
      "val 5 epoch loss: 0.095  acc:96.32 \n",
      "time used : 30.64425110022227 minutes\n"
     ]
    }
   ],
   "source": [
    "train_loss_batch=[]# 记录训练过程中的损失值\n",
    "val_loss_batch=[]\n",
    "save_every=1 #每一个epoch保存一下网络参数值\n",
    "epochs=5 \n",
    "print_per_batches=20\n",
    "\n",
    "# 开始训练，并记录训练消耗的时间\n",
    "time_open=time.time()\n",
    "train()\n",
    "time_end=time.time()\n",
    "time_used=time_end-time_open\n",
    "print('time used : {} minutes'.format(time_used/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出训练集的损失函数曲线图\n",
    "x=range(1,len(train_loss_batch)+1)\n",
    "plt.plot(x,train_loss_batch,label='train loss')\n",
    "plt.plot(x,val_loss_batch,label='val loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 用VGG网络迁移学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=models.vgg16(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "model.classifier=nn.Sequential(nn.Linear(25088,2048),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Dropout(p=0.5),\n",
    "                              nn.Linear(2048,2048),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Dropout(p=0.5),\n",
    "                              nn.Linear(2048,2))\n",
    "model=model.cuda()\n",
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.classifier.parameters(),lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_batch=[]# 记录训练过程中的损失值\n",
    "val_loss_batch=[]\n",
    "save_every=1 #每一个epoch保存一下网络参数值\n",
    "epochs=5 \n",
    "print_per_batches=20\n",
    "\n",
    "# 开始训练，并记录训练消耗的时间\n",
    "time_open=time.time()\n",
    "train()\n",
    "time_end=time.time()\n",
    "time_used=time_end-time_open\n",
    "print('time used : {} minutes'.format(time_used/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出训练集的损失函数曲线图\n",
    "x=range(1,len(train_loss_batch)+1)\n",
    "plt.plot(x,train_loss_batch,label='train loss')\n",
    "plt.plot(x,val_loss_batch,label='val loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 用ResNet-18网络迁移学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "dim_in = model.fc.in_features\n",
    "model.fc = nn.Linear(dim_in,2)\n",
    "model=model.cuda()\n",
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.fc.parameters(),lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1    10] loss: 0.698\n",
      "[1    20] loss: 0.640\n",
      "[1    30] loss: 0.560\n",
      "[1    40] loss: 0.537\n",
      "[1    50] loss: 0.505\n",
      "[1    60] loss: 0.478\n",
      "[1    70] loss: 0.446\n",
      "[1    80] loss: 0.402\n",
      "[1    90] loss: 0.394\n",
      "[1   100] loss: 0.368\n",
      "[1   110] loss: 0.358\n",
      "[1   120] loss: 0.333\n",
      "[1   130] loss: 0.301\n",
      "[1   140] loss: 0.295\n",
      "[1   150] loss: 0.303\n",
      "[1   160] loss: 0.297\n",
      "[1   170] loss: 0.273\n",
      "[1   180] loss: 0.254\n",
      "[1   190] loss: 0.246\n",
      "[1   200] loss: 0.235\n",
      "[1   210] loss: 0.254\n",
      "[1   220] loss: 0.229\n",
      "[1   230] loss: 0.250\n",
      "[1   240] loss: 0.220\n",
      "[1   250] loss: 0.216\n",
      "[1   260] loss: 0.211\n",
      "[1   270] loss: 0.199\n",
      "[1   280] loss: 0.206\n",
      "[1   290] loss: 0.202\n",
      "[1   300] loss: 0.212\n",
      "[1   310] loss: 0.198\n",
      "[1   320] loss: 0.178\n",
      "[1   330] loss: 0.174\n",
      "[1   340] loss: 0.182\n",
      "[1   350] loss: 0.172\n",
      "train 1 epoch  acc: 90.28\n",
      "correct : tensor(2409, device='cuda:0')  val_total : 2500\n",
      "val 1 epoch loss: 0.163  acc:96.36 \n",
      "[2    10] loss: 0.165\n",
      "[2    20] loss: 0.170\n",
      "[2    30] loss: 0.177\n",
      "[2    40] loss: 0.140\n",
      "[2    50] loss: 0.181\n",
      "[2    60] loss: 0.166\n",
      "[2    70] loss: 0.152\n",
      "[2    80] loss: 0.170\n",
      "[2    90] loss: 0.155\n",
      "[2   100] loss: 0.148\n",
      "[2   110] loss: 0.155\n",
      "[2   120] loss: 0.169\n",
      "[2   130] loss: 0.131\n",
      "[2   140] loss: 0.153\n",
      "[2   150] loss: 0.138\n",
      "[2   160] loss: 0.153\n",
      "[2   170] loss: 0.155\n",
      "[2   180] loss: 0.136\n",
      "[2   190] loss: 0.141\n",
      "[2   200] loss: 0.128\n",
      "[2   210] loss: 0.138\n",
      "[2   220] loss: 0.142\n",
      "[2   230] loss: 0.141\n",
      "[2   240] loss: 0.131\n",
      "[2   250] loss: 0.130\n",
      "[2   260] loss: 0.124\n",
      "[2   270] loss: 0.136\n",
      "[2   280] loss: 0.143\n",
      "[2   290] loss: 0.128\n",
      "[2   300] loss: 0.131\n",
      "[2   310] loss: 0.136\n",
      "[2   320] loss: 0.132\n",
      "[2   330] loss: 0.140\n",
      "[2   340] loss: 0.123\n",
      "[2   350] loss: 0.116\n",
      "train 2 epoch  acc: 96.28\n",
      "correct : tensor(2429, device='cuda:0')  val_total : 2500\n",
      "val 2 epoch loss: 0.108  acc:97.16 \n",
      "[3    10] loss: 0.122\n",
      "[3    20] loss: 0.114\n",
      "[3    30] loss: 0.111\n",
      "[3    40] loss: 0.119\n",
      "[3    50] loss: 0.122\n",
      "[3    60] loss: 0.106\n",
      "[3    70] loss: 0.118\n",
      "[3    80] loss: 0.140\n",
      "[3    90] loss: 0.134\n",
      "[3   100] loss: 0.109\n",
      "[3   110] loss: 0.113\n",
      "[3   120] loss: 0.112\n",
      "[3   130] loss: 0.122\n",
      "[3   140] loss: 0.112\n",
      "[3   150] loss: 0.102\n",
      "[3   160] loss: 0.109\n",
      "[3   170] loss: 0.104\n",
      "[3   180] loss: 0.132\n",
      "[3   190] loss: 0.111\n",
      "[3   200] loss: 0.102\n",
      "[3   210] loss: 0.105\n",
      "[3   220] loss: 0.093\n",
      "[3   230] loss: 0.093\n",
      "[3   240] loss: 0.093\n",
      "[3   250] loss: 0.105\n",
      "[3   260] loss: 0.113\n",
      "[3   270] loss: 0.095\n",
      "[3   280] loss: 0.103\n",
      "[3   290] loss: 0.125\n",
      "[3   300] loss: 0.100\n",
      "[3   310] loss: 0.107\n",
      "[3   320] loss: 0.114\n",
      "[3   330] loss: 0.110\n",
      "[3   340] loss: 0.106\n",
      "[3   350] loss: 0.113\n",
      "train 3 epoch  acc: 96.84\n",
      "correct : tensor(2438, device='cuda:0')  val_total : 2500\n",
      "val 3 epoch loss: 0.090  acc:97.52 \n",
      "[4    10] loss: 0.093\n",
      "[4    20] loss: 0.106\n",
      "[4    30] loss: 0.103\n",
      "[4    40] loss: 0.110\n",
      "[4    50] loss: 0.106\n",
      "[4    60] loss: 0.110\n",
      "[4    70] loss: 0.097\n",
      "[4    80] loss: 0.097\n",
      "[4    90] loss: 0.100\n",
      "[4   100] loss: 0.106\n",
      "[4   110] loss: 0.121\n",
      "[4   120] loss: 0.087\n",
      "[4   130] loss: 0.086\n",
      "[4   140] loss: 0.100\n",
      "[4   150] loss: 0.102\n",
      "[4   160] loss: 0.103\n",
      "[4   170] loss: 0.100\n",
      "[4   180] loss: 0.098\n",
      "[4   190] loss: 0.079\n",
      "[4   200] loss: 0.113\n",
      "[4   210] loss: 0.111\n",
      "[4   220] loss: 0.101\n",
      "[4   230] loss: 0.081\n",
      "[4   240] loss: 0.085\n",
      "[4   250] loss: 0.099\n",
      "[4   260] loss: 0.086\n",
      "[4   270] loss: 0.089\n",
      "[4   280] loss: 0.083\n",
      "[4   290] loss: 0.081\n",
      "[4   300] loss: 0.087\n",
      "[4   310] loss: 0.083\n",
      "[4   320] loss: 0.091\n",
      "[4   330] loss: 0.111\n",
      "[4   340] loss: 0.087\n",
      "[4   350] loss: 0.095\n",
      "train 4 epoch  acc: 97.04\n",
      "correct : tensor(2441, device='cuda:0')  val_total : 2500\n",
      "val 4 epoch loss: 0.084  acc:97.64 \n",
      "[5    10] loss: 0.085\n",
      "[5    20] loss: 0.106\n",
      "[5    30] loss: 0.079\n",
      "[5    40] loss: 0.080\n",
      "[5    50] loss: 0.083\n",
      "[5    60] loss: 0.097\n",
      "[5    70] loss: 0.102\n",
      "[5    80] loss: 0.089\n",
      "[5    90] loss: 0.092\n",
      "[5   100] loss: 0.090\n",
      "[5   110] loss: 0.097\n",
      "[5   120] loss: 0.085\n",
      "[5   130] loss: 0.087\n",
      "[5   140] loss: 0.069\n",
      "[5   150] loss: 0.083\n",
      "[5   160] loss: 0.096\n",
      "[5   170] loss: 0.098\n",
      "[5   180] loss: 0.078\n",
      "[5   190] loss: 0.083\n",
      "[5   200] loss: 0.093\n",
      "[5   210] loss: 0.084\n",
      "[5   220] loss: 0.106\n",
      "[5   230] loss: 0.103\n",
      "[5   240] loss: 0.082\n",
      "[5   250] loss: 0.099\n",
      "[5   260] loss: 0.074\n",
      "[5   270] loss: 0.095\n",
      "[5   280] loss: 0.088\n",
      "[5   290] loss: 0.089\n",
      "[5   300] loss: 0.085\n",
      "[5   310] loss: 0.068\n",
      "[5   320] loss: 0.087\n",
      "[5   330] loss: 0.064\n",
      "[5   340] loss: 0.086\n",
      "[5   350] loss: 0.085\n",
      "train 5 epoch  acc: 97.20\n",
      "correct : tensor(2443, device='cuda:0')  val_total : 2500\n",
      "val 5 epoch loss: 0.072  acc:97.72 \n",
      "time used : 27.703934252262115 minutes\n"
     ]
    }
   ],
   "source": [
    "train_loss_batch=[]# 记录训练过程中的损失值\n",
    "val_loss_batch=[]\n",
    "save_every=1 #每一个epoch保存一下网络参数值\n",
    "epochs=5 \n",
    "print_per_batches=20\n",
    "\n",
    "# 开始训练，并记录训练消耗的时间\n",
    "time_open=time.time()\n",
    "train()\n",
    "time_end=time.time()\n",
    "time_used=time_end-time_open\n",
    "print('time used : {} minutes'.format(time_used/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出训练集的损失函数曲线图\n",
    "x=range(1,len(train_loss_batch)+1)\n",
    "plt.plot(x,train_loss_batch,label='train loss')\n",
    "plt.plot(x,val_loss_batch,label='val loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 用ResNet-50网络迁移学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=models.resnet50(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "dim_in = model.fc.in_features\n",
    "model.fc = nn.Linear(dim_in,2)\n",
    "model=model.cuda()\n",
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.fc.parameters(),lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1    10] loss: 0.655\n",
      "[1    20] loss: 0.568\n",
      "[1    30] loss: 0.497\n",
      "[1    40] loss: 0.446\n",
      "[1    50] loss: 0.407\n",
      "[1    60] loss: 0.361\n",
      "[1    70] loss: 0.316\n",
      "[1    80] loss: 0.301\n",
      "[1    90] loss: 0.277\n",
      "[1   100] loss: 0.261\n",
      "[1   110] loss: 0.243\n",
      "[1   120] loss: 0.231\n",
      "[1   130] loss: 0.225\n",
      "[1   140] loss: 0.212\n",
      "[1   150] loss: 0.203\n",
      "[1   160] loss: 0.182\n",
      "[1   170] loss: 0.188\n",
      "[1   180] loss: 0.192\n",
      "[1   190] loss: 0.191\n",
      "[1   200] loss: 0.169\n",
      "[1   210] loss: 0.164\n",
      "[1   220] loss: 0.168\n",
      "[1   230] loss: 0.152\n",
      "[1   240] loss: 0.148\n",
      "[1   250] loss: 0.146\n",
      "[1   260] loss: 0.152\n",
      "[1   270] loss: 0.135\n",
      "[1   280] loss: 0.146\n",
      "[1   290] loss: 0.131\n",
      "[1   300] loss: 0.136\n",
      "[1   310] loss: 0.117\n",
      "[1   320] loss: 0.127\n",
      "[1   330] loss: 0.135\n",
      "[1   340] loss: 0.112\n",
      "[1   350] loss: 0.131\n",
      "train 1 epoch  acc: 94.67\n",
      "correct : tensor(2426, device='cuda:0')  val_total : 2500\n",
      "val 1 epoch loss: 0.111  acc:97.04 \n",
      "[2    10] loss: 0.114\n",
      "[2    20] loss: 0.109\n",
      "[2    30] loss: 0.131\n",
      "[2    40] loss: 0.119\n",
      "[2    50] loss: 0.119\n",
      "[2    60] loss: 0.109\n",
      "[2    70] loss: 0.099\n",
      "[2    80] loss: 0.119\n",
      "[2    90] loss: 0.107\n",
      "[2   100] loss: 0.108\n",
      "[2   110] loss: 0.099\n",
      "[2   120] loss: 0.095\n",
      "[2   130] loss: 0.118\n",
      "[2   140] loss: 0.120\n",
      "[2   150] loss: 0.101\n",
      "[2   160] loss: 0.099\n",
      "[2   170] loss: 0.095\n",
      "[2   180] loss: 0.098\n",
      "[2   190] loss: 0.129\n",
      "[2   200] loss: 0.106\n",
      "[2   210] loss: 0.102\n",
      "[2   220] loss: 0.089\n",
      "[2   230] loss: 0.104\n",
      "[2   240] loss: 0.118\n",
      "[2   250] loss: 0.085\n",
      "[2   260] loss: 0.090\n",
      "[2   270] loss: 0.096\n",
      "[2   280] loss: 0.086\n",
      "[2   290] loss: 0.084\n",
      "[2   300] loss: 0.105\n",
      "[2   310] loss: 0.081\n",
      "[2   320] loss: 0.078\n",
      "[2   330] loss: 0.076\n",
      "[2   340] loss: 0.074\n",
      "[2   350] loss: 0.089\n",
      "train 2 epoch  acc: 97.31\n",
      "correct : tensor(2443, device='cuda:0')  val_total : 2500\n",
      "val 2 epoch loss: 0.082  acc:97.72 \n",
      "[3    10] loss: 0.092\n",
      "[3    20] loss: 0.095\n",
      "[3    30] loss: 0.073\n",
      "[3    40] loss: 0.104\n",
      "[3    50] loss: 0.101\n",
      "[3    60] loss: 0.089\n",
      "[3    70] loss: 0.095\n",
      "[3    80] loss: 0.084\n",
      "[3    90] loss: 0.085\n",
      "[3   100] loss: 0.085\n",
      "[3   110] loss: 0.077\n",
      "[3   120] loss: 0.065\n",
      "[3   130] loss: 0.088\n",
      "[3   140] loss: 0.096\n",
      "[3   150] loss: 0.077\n",
      "[3   160] loss: 0.076\n",
      "[3   170] loss: 0.070\n",
      "[3   180] loss: 0.087\n",
      "[3   190] loss: 0.084\n",
      "[3   200] loss: 0.069\n",
      "[3   210] loss: 0.076\n",
      "[3   220] loss: 0.068\n",
      "[3   230] loss: 0.087\n",
      "[3   240] loss: 0.087\n",
      "[3   250] loss: 0.072\n",
      "[3   260] loss: 0.075\n",
      "[3   270] loss: 0.079\n",
      "[3   280] loss: 0.071\n",
      "[3   290] loss: 0.075\n",
      "[3   300] loss: 0.090\n",
      "[3   310] loss: 0.086\n",
      "[3   320] loss: 0.064\n",
      "[3   330] loss: 0.087\n",
      "[3   340] loss: 0.083\n",
      "[3   350] loss: 0.080\n",
      "train 3 epoch  acc: 97.44\n",
      "correct : tensor(2451, device='cuda:0')  val_total : 2500\n",
      "val 3 epoch loss: 0.066  acc:98.04 \n",
      "[4    10] loss: 0.082\n",
      "[4    20] loss: 0.094\n",
      "[4    30] loss: 0.080\n",
      "[4    40] loss: 0.089\n",
      "[4    50] loss: 0.063\n",
      "[4    60] loss: 0.076\n",
      "[4    70] loss: 0.083\n",
      "[4    80] loss: 0.075\n",
      "[4    90] loss: 0.075\n",
      "[4   100] loss: 0.077\n",
      "[4   110] loss: 0.082\n",
      "[4   120] loss: 0.086\n",
      "[4   130] loss: 0.081\n",
      "[4   140] loss: 0.077\n",
      "[4   150] loss: 0.065\n",
      "[4   160] loss: 0.085\n",
      "[4   170] loss: 0.059\n",
      "[4   180] loss: 0.065\n",
      "[4   190] loss: 0.067\n",
      "[4   200] loss: 0.075\n",
      "[4   210] loss: 0.080\n",
      "[4   220] loss: 0.065\n",
      "[4   230] loss: 0.061\n",
      "[4   240] loss: 0.056\n",
      "[4   250] loss: 0.055\n",
      "[4   260] loss: 0.063\n",
      "[4   270] loss: 0.073\n",
      "[4   280] loss: 0.054\n",
      "[4   290] loss: 0.057\n",
      "[4   300] loss: 0.056\n",
      "[4   310] loss: 0.055\n",
      "[4   320] loss: 0.067\n",
      "[4   330] loss: 0.055\n",
      "[4   340] loss: 0.056\n",
      "[4   350] loss: 0.080\n",
      "train 4 epoch  acc: 97.70\n",
      "correct : tensor(2458, device='cuda:0')  val_total : 2500\n",
      "val 4 epoch loss: 0.056  acc:98.32 \n",
      "[5    10] loss: 0.080\n",
      "[5    20] loss: 0.074\n",
      "[5    30] loss: 0.058\n",
      "[5    40] loss: 0.057\n",
      "[5    50] loss: 0.060\n",
      "[5    60] loss: 0.066\n",
      "[5    70] loss: 0.077\n",
      "[5    80] loss: 0.057\n",
      "[5    90] loss: 0.066\n",
      "[5   100] loss: 0.058\n",
      "[5   110] loss: 0.072\n",
      "[5   120] loss: 0.064\n",
      "[5   130] loss: 0.058\n",
      "[5   140] loss: 0.068\n",
      "[5   150] loss: 0.076\n",
      "[5   160] loss: 0.072\n",
      "[5   170] loss: 0.068\n",
      "[5   180] loss: 0.059\n",
      "[5   190] loss: 0.061\n",
      "[5   200] loss: 0.067\n",
      "[5   210] loss: 0.058\n",
      "[5   220] loss: 0.053\n",
      "[5   230] loss: 0.067\n",
      "[5   240] loss: 0.048\n",
      "[5   250] loss: 0.073\n",
      "[5   260] loss: 0.064\n",
      "[5   270] loss: 0.054\n",
      "[5   280] loss: 0.071\n",
      "[5   290] loss: 0.076\n",
      "[5   300] loss: 0.057\n",
      "[5   310] loss: 0.051\n",
      "[5   320] loss: 0.062\n",
      "[5   330] loss: 0.085\n",
      "[5   340] loss: 0.080\n",
      "[5   350] loss: 0.078\n",
      "train 5 epoch  acc: 97.81\n",
      "correct : tensor(2451, device='cuda:0')  val_total : 2500\n",
      "val 5 epoch loss: 0.057  acc:98.04 \n",
      "time used : 63.96972184578578 minutes\n"
     ]
    }
   ],
   "source": [
    "train_loss_batch=[]# 记录训练过程中的损失值\n",
    "val_loss_batch=[]\n",
    "save_every=1 #每一个epoch保存一下网络参数值\n",
    "epochs=5 \n",
    "print_per_batches=20\n",
    "\n",
    "# 开始训练，并记录训练消耗的时间\n",
    "time_open=time.time()\n",
    "train()\n",
    "time_end=time.time()\n",
    "time_used=time_end-time_open\n",
    "print('time used : {} minutes'.format(time_used/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出训练集的损失函数曲线图\n",
    "x=range(1,len(train_loss_batch)+1)\n",
    "plt.plot(x,train_loss_batch,label='train loss')\n",
    "plt.plot(x,val_loss_batch,label='val loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
